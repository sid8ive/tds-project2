# -*- coding: utf-8 -*-
"""TDSP2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13NuNzjUDSbILD5RflWtGmUXPk1R6ximW

#TDS Project 2
"""

# import os

# # Set the environment variable in the session (this can be done in a secure way)
# os.environ['AIPROXY_TOKEN'] = 'provide_token'

import os
import requests
import pandas as pd
import numpy as np
import seaborn as sns
import logging
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from scipy import stats
from sklearn.cluster import KMeans
from scipy.stats import zscore, skew, kurtosis
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import statsmodels.api as sm
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
import re

class DataAnalyzer:
    def __init__(self, api_token=None, api_base="https://aiproxy.sanand.workers.dev/openai/v1"):
        self.api_headers, self.api_base = self._initialize_openai(api_token, api_base)
        self.logger = logging.getLogger(__name__)

    def _initialize_openai(self, api_token=None, api_base="https://aiproxy.sanand.workers.dev/openai/v1"):
        if not api_token:
            api_token = os.getenv("AIPROXY_TOKEN")

        if api_token:
            return {
                "Authorization": f"Bearer {api_token}",
                "Content-Type": "application/json"
            }, api_base

        print("Warning: API Token is missing.")
        return None, None

    def load_csv(self, file_path):
        """
        Load a CSV file into a DataFrame.
        """
        try:
          data = pd.read_csv(file_path, encoding='unicode_escape')
          print("File loaded successfully!")
          return data
        except FileNotFoundError:
          raise FileNotFoundError(f"CSV file ({file_path}) not found. Please check the file path.")
        except Exception as e:
           raise Exception(f"Error loading CSV file: {e}")
        return None


    def detect_outliers(self, data, contamination=0.05):
        """
        Detects outliers in the data using Isolation Forest.

        Args:
            data (pd.DataFrame): Input data.
            contamination (float, optional): Proportion of outliers. Defaults to 0.05.

        Returns:
            pd.DataFrame: Dataframe containing outliers.
        """

        model = IsolationForest(contamination=contamination)
        outliers = model.fit_predict(data)
        return data[outliers == -1]


    def data_cleaning(self, data, advanced_cleaning=True):
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        categorical_columns = data.select_dtypes(include=['object']).columns
        numeric_imputer = SimpleImputer(strategy='mean')
        categorical_imputer = SimpleImputer(strategy='most_frequent')

        data[numeric_columns] = numeric_imputer.fit_transform(data[numeric_columns])
        data[categorical_columns] = categorical_imputer.fit_transform(data[categorical_columns])
        data = data.drop_duplicates()

        if advanced_cleaning:
            if len(numeric_columns) > 0:
                iso_forest = IsolationForest(contamination=0.1)
                anomaly_labels = iso_forest.fit_predict(data[numeric_columns])
                data = data[anomaly_labels != -1]
            scaler = StandardScaler()
            data[numeric_columns] = scaler.fit_transform(data[numeric_columns])

        return data

    def data_quality_report(self, data):
        """
        Generate comprehensive data quality report.
        """
        report = {
            "Total Rows": len(data),
            "Total Columns": len(data.columns),
            "Missing Values": data.isnull().sum().to_dict(),
            "Data Types": data.dtypes.to_dict(),
            "Unique Values": {col: data[col].nunique() for col in data.columns}
        }

        # Print detailed report
        print("\n--- Data Quality Report ---")
        for key, value in report.items():
            print(f"{key}:")
            if isinstance(value, dict):
                for subkey, subvalue in value.items():
                    print(f"  {subkey}: {subvalue}")
            else:
                print(f"  {value}")

        return report

    def statistical_analysis(self, data):
        """
        Perform comprehensive statistical analysis.
        """
        # Select numeric columns
        numeric_columns = data.select_dtypes(include=[np.number]).columns

        # Statistical summary
        statistical_summary = {
            "Descriptive Statistics": data[numeric_columns].describe().to_dict(),
            "Skewness": {col: skew(data[col]) for col in numeric_columns},
            "Kurtosis": {col: kurtosis(data[col]) for col in numeric_columns}
        }

        return statistical_summary




    def content_summary(self, data):
        """
        Generate comprehensive content summary using OpenAI API.
        """

        if not self.api_headers or not self.api_base:
          self.logger.warning("API not initialized. Cannot generate summary.")
          return None

        # Convert data to string representation
        data_str = data.to_string()

        url = f"{self.api_base}/chat/completions"
        data_payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": "You are an expert data analyst providing comprehensive insights."},
                {"role": "user", "content": f"""
                Provide a detailed summary and key insights from this dataset:
                {data_str[:10000]}  # Limit to first 10000 characters

                Key points to cover:
                - Overall dataset composition
                - Key variables and their significance
                - Any notable patterns or trends
                - Potential insights or implications
                """}
            ]
        }

        try:
            response = requests.post(url, headers=self.api_headers, json=data_payload)
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content'].strip()
        except requests.exceptions.RequestException as e:
            print(f"Content summary error: {e}")
            return None

    def advanced_visualization(self, data, file_path):
      numeric_columns = data.select_dtypes(include=[np.number]).columns
      if len(numeric_columns) >= 2:
        # Pairplot for numeric columns
        plt.figure(figsize=(12, 10), dpi=72)
        sns.pairplot(data[numeric_columns], diag_kind='kde')
        plt.suptitle('Pairplot of Numeric Variables', y=1.02)
        plt.tight_layout()
        plt.savefig(os.path.dirname(file_path)+'/pairplot.png',dpi=72)  # Save plot first
        plt.close()  # Close the plot


    def dimensionality_reduction(self, data, n_components=2):
        """
        Perform PCA for dimensionality reduction.

        Args:
            data (DataFrame): Input data
            n_components (int): Number of principal components

        Returns:
            DataFrame with reduced dimensions
        """
        numeric_columns = data.select_dtypes(include=[np.number]).columns

        if len(numeric_columns) <= n_components:
            self.logger.warning("Not enough numeric columns for meaningful dimensionality reduction.")
            return data

        pca = PCA(n_components=min(n_components, len(numeric_columns)))
        reduced_data = pca.fit_transform(data[numeric_columns])

        # Create new DataFrame with reduced dimensions
        reduced_df = pd.DataFrame(
            reduced_data,
            columns=[f'PC{i+1}' for i in range(reduced_data.shape[1])]
        )

        # Print variance explained
        explained_variance = pca.explained_variance_ratio_
        self.logger.info(f"Variance Explained: {explained_variance}")

        return reduced_df, explained_variance


    def kmeans_clustering(self, data,file_path, n_clusters=3, advanced_clustering=True):
        """
        Enhanced clustering with optional advanced techniques.

        Args:
            data (DataFrame): Input data
            n_clusters (int): Number of clusters for KMeans
            advanced_clustering (bool): Enable DBSCAN as alternative
        """
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) < 2:
            self.logger.warning("Not enough numeric columns for clustering.")
            return data

        # Standardize the data
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data[numeric_columns])

        # Clustering algorithm selection
        if advanced_clustering and len(scaled_data) > n_clusters:
            try:
                # Try DBSCAN for more adaptive clustering
                clusterer = DBSCAN(eps=0.5, min_samples=5)
                clusters = clusterer.fit_predict(scaled_data)
            except Exception:
                # Fallback to KMeans
                clusterer = KMeans(n_clusters=n_clusters, random_state=42)
                clusters = clusterer.fit_predict(scaled_data)
        else:
            # Standard KMeans
            clusterer = KMeans(n_clusters=n_clusters, random_state=42)
            clusters = clusterer.fit_predict(scaled_data)

        data['Cluster'] = clusters

        # Dimensionality reduction for visualization
        pca = PCA(n_components=2)
        reduced_data = pca.fit_transform(scaled_data)

        # Plot clusters
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters, cmap='viridis')
        plt.title(f"Advanced Clustering Visualization")
        plt.colorbar(scatter)
        plt.tight_layout()


        # Save the plot to a PNG file
        plt.savefig(os.path.dirname(file_path)+"/kmeans_clustering.png")
        plt.close()  # Close the figure to avoid memory issues



        return data

    def trend_analysis(self, data, file_path):
        """
        Perform trend analysis using Holt-Winters Exponential Smoothing for time series data.
        """
        # Identify columns that may be time-based
        time_column = None
        for col in data.columns:
            if any(keyword in col.lower() for keyword in ['date', 'time', 'timestamp', 'datetime']):
                time_column = col
                break

        if time_column is None:
            print("No time-related column found for trend analysis.")
            return

        # Try to convert identified column to datetime
        data[time_column] = pd.to_datetime(data[time_column], errors='coerce')
        data.dropna(subset=[time_column], inplace=True)

        # Ask the user to choose a column for trend analysis
        print(f"Trend analysis will be performed on the column '{time_column}'.")
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) == 0:
            print("No numeric columns found for trend analysis.")
            return

        value_column = numeric_columns[0]  # Assuming the first numeric column is the one to analyze

        # Set the time column as index and perform trend analysis
        data.set_index(time_column, inplace=True)
        data = data[[value_column]].dropna()

        model = ExponentialSmoothing(data, trend='add', seasonal='add', seasonal_periods=12)
        model_fit = model.fit()
        forecast = model_fit.forecast(steps=12)

        plt.figure(figsize=(12, 6),dpi=72)
        plt.plot(data.index, data[value_column], label="Actual")
        plt.plot(forecast.index, forecast, label="Forecast", color='red')
        plt.title(f"Trend Analysis for {value_column} using Holt-Winters Method")
        plt.legend()
        plt.tight_layout()

        plt.savefig(os.path.dirname(file_path)+"/trend_analysis.png",dpi=72)
        plt.close()  # Close the figure to avoid memory issues

    def word_cloud(self, data, file_path):
        """
        Generate a word cloud from the entire content of the CSV file.
        Excludes URLs in the word cloud generation.
        """
        text_data = " ".join(data.select_dtypes(include=['object']).apply(lambda x: " ".join(x.astype(str)), axis=1).values.flatten())
        text_data = re.sub(r'http[s]?://\S+', '', text_data)

        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)

        plt.figure(figsize=(10, 8),dpi=72)
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.tight_layout()


        # Save the plot to a PNG file
        plt.savefig(os.path.dirname(file_path)+"/word_cloud.png",dpi=72)
        plt.close()  # Close the figure to avoid memory issues

    def generate_summary(self, data_file_path, summary_file_path):
      """
      Create a output.md file that dumps all analysis outputs.
      This function will call the main analysis and save the results to a markdown file.

      Args:
          file_path (str): Path to the input data file (CSV).
          output_path (str): Path where the output.md file will be saved.
      """
      data = self.load_csv(data_file_path)
      if data is None:
          return

      with open(summary_file_path, "w") as summary_file_path:
          summary_file_path.write(f"# Data Analysis Report for file {data_file_path}\n\n")

          # Data Quality Report
          report = self.data_quality_report(data)
          summary_file_path.write("## Data Quality Report\n")
          for key, value in report.items():
              summary_file_path.write(f"**{key}:**\n")
              if isinstance(value, dict):
                  for subkey, subvalue in value.items():
                      summary_file_path.write(f"- {subkey}: {subvalue}\n")
              else:
                  summary_file_path.write(f"- {value}\n")
          summary_file_path.write("\n")



          # Statistical Analysis
          statistical_summary = self.statistical_analysis(data)
          summary_file_path.write("## Statistical Analysis\n")
          for category, stats in statistical_summary.items():
              summary_file_path.write(f"### {category}\n")
              if category == "Descriptive Statistics":
                  table_header = "| Column | Count | Mean | Std | Min | 25% | 50% | 75% | Max |"
                  table_header += "\n|--------|-------|------|-----|-----|-----|-----|-----|-----|"
                  table_rows = []
                  for col, values in stats.items():
                      row = f"| {col} | {values['count']} | {values['mean']} | {values['std']} | {values['min']} | {values['25%']} | {values['50%']} | {values['75%']} | {values['max']} |"
                      table_rows.append(row)
                  table_content = "\n".join(table_rows)
                  summary_file_path.write(table_header + "\n" + table_content + "\n")

              else:
                  for subkey, subvalue in stats.items():
                      summary_file_path.write(f"- {subkey}: {subvalue}\n")

          
          # Advanced Visualization - Save figures instead of printing to screen
          summary_file_path.write("## Visualizations\n")
          self.advanced_visualization(data, data_file_path)  # Show plot
          
          image_path = os.path.join(os.path.dirname(data_file_path), "pairplot.png")
          summary_file_path.write(f"![Pairplot]({image_path})\n")

          # Clean the Data with advanced techniques
          cleaned_data = self.data_cleaning(data, advanced_cleaning=True)

          # outliers = self.detect_outliers(data)
          # summary_file_path.write("## Outliers\n")
          # summary_file_path.write(f"Outliers: {outliers}\n\n")

          # Dimensionality Reduction Report
          reduced_data,_ = self.dimensionality_reduction(data)
          summary_file_path.write("## Dimensionality Reduction\n")
          summary_file_path.write(f"Reduced Data Shape: {reduced_data.shape}\n\n")

          # KMeans Clustering Results
          clustered_data = self.kmeans_clustering(data, data_file_path)
          summary_file_path.write("## KMeans Clustering\n")
          summary_file_path.write(f"Clustered Data:\n{clustered_data.head()}\n\n")

          image_path = os.path.join(os.path.dirname(data_file_path), "kmean_clustering.png")
          summary_file_path.write(f"![KMean Clustering]({image_path})\n")

          # Trend Analysis - Save plot for later reference
          self.trend_analysis(data, data_file_path)

          image_path = os.path.join(os.path.dirname(data_file_path), "trend_analysis.png")
          summary_file_path.write(f"![Trend Analyses]({image_path})\n")

          
          # Word Cloud Image
          self.word_cloud(data, data_file_path)
          image_path = os.path.join(os.path.dirname(data_file_path), "word_cloud.png")
          summary_file_path.write(f"![Word Cloud]({image_path})\n")

          
          # Content Summary from OpenAI
          summary = self.content_summary(data)
          if summary:
              summary_file_path.write("## Content Summary\n")
              summary_file_path.write(summary + "\n")

      print(f"Summary file created at {summary_file_path}")



    def main_analysis(self, file_path):
        """
        Comprehensive data analysis pipeline.
        """
        # Load data
        data = self.load_csv(file_path)


        if data is not None:
            # Data Quality Report
            self.data_quality_report(data)

            # Statistical Analysis
            self.statistical_analysis(data)

            # Advanced Visualization
            self.advanced_visualization(data)

            # Clean the Data with advanced techniques
            cleaned_data = self.data_cleaning(data, advanced_cleaning=True)

            # Dimensionality Reduction
            reduced_data = self.dimensionality_reduction(cleaned_data)

            # Perform KMeans Clustering with advanced options
            clustered_data = self.kmeans_clustering(cleaned_data, advanced_clustering=True)

            # Trend Analysis
            self.trend_analysis(data)

            # Word Analysis
            self.word_cloud(data)

            # Content Summary
            summary = self.content_summary(data)
            if summary:
                print("\n--- Dataset Content Summary ---")
                print(summary)



    def generate_story_from_summary(self, summary_file_path, story_file_path):
      """
      Generate a structured story about analysis based on output.md and include images in markdown format.

      The story includes:
          1. The data received
          2. The analysis carried out
          3. The insights discovered
          4. The implications of the findings

      Any images referenced in the output.md file will be embedded in the story.

      Args:
      - output_path (str): Path to the output.md file.
      - story_path (str): Path where the generated story will be saved (default is 'README.md').

      Returns:
      - story (str): Generated story or error message.
      """
      if not self.api_headers or not self.api_base:
          print("API not initialized. Cannot generate story.")
          return None

      try:
          # Step 1: Read the output.md content
          with open(summary_file_path, 'r') as file:
              output_content = file.read()

          # Step 2: Extract image references from the output.md file
          image_references = re.findall(r'!\[.*?\]\((.*?)\)', output_content)  # Markdown image syntax
          raw_image_urls = re.findall(r'(https?://\S+\.(?:png|jpg|jpeg|gif|bmp|svg))', output_content)  # Raw URLs
          all_images = list(set(image_references + raw_image_urls))  # Combine and deduplicate

          # Step 3: Prepare the prompt for OpenAI
          prompt = f"""
          Based on the following output file content, generate a story in four sections:
          1. The data received: Briefly describe the type of data and its source.
          2. The analysis carried out: Outline the methods, tools, or techniques used for analysis.
          3. The insights discovered: Summarize the key findings or trends identified in the analysis.
          4. The implications of your findings: Describe the potential actions or applications based on the insights.

          Include the following images in relevant sections of the story:
          {all_images}

          Output file Content:
          {output_content[:10000]}
          """

          # Step 4: Prepare the data payload for OpenAI API
          url = f"{self.api_base}/chat/completions"
          data_payload = {
              "model": "gpt-4o-mini",
              "messages": [
                  {"role": "system", "content": "You are a structured storyteller. Organize the story into four clear sections and embed the images in markdown format."},
                  {"role": "user", "content": prompt}
              ]
          }

          # Step 5: Make the request to the OpenAI API
          response = requests.post(url, headers=self.api_headers, json=data_payload)
          response.raise_for_status()

          # Step 6: Parse the generated story
          story = response.json()['choices'][0]['message']['content'].strip()

          # Step 7: Write the generated story with images to the specified output file
          with open(story_file_path, 'w') as output_file:
              output_file.write("# Analysis Story with Images\n\n")
              output_file.write(story)

          print(f"Story successfully generated and saved to {story_file_path}")

          return story

      except requests.exceptions.RequestException as e:
        print(f"Story generation error: {e}")
        return None
      except Exception as e:
        print(f"An error occurred during story generation: {e}")
        return None


def main():
    analyzer = DataAnalyzer()
    data_file = "python/tdsproject2/eateries/eateries.csv"
    folder_name = os.path.dirname(data_file);

    summary_file = folder_name+"/summary.md"
    story_file =folder_name+"/README.md"

    #analyzer.main_analysis("happiness.csv")  # Replace with your actual CSV file path
    analyzer.generate_summary(data_file, summary_file)  # Replace with your actual file path

    
    analyzer.generate_story_from_summary(summary_file, story_file)


if __name__ == "__main__":
    main()
